---
layout: post
comments: true
title: "论文摘趣 (1)"
excerpt: "论文摘趣第一期，CVPR 2017 专题，一起来看看我在夏威夷看到的有趣的论文！"
date: 2017-07-28
mathjax: true
---


论文摘取的第一期，我们来关注一下计算机视觉的“春晚” -- CVPR。今年的CVPR在夏威夷召开，无论是从录取论文数还是到会人数来看都是空前的盛大。作为我第一次参加的学术会议以及第一次去的夏威夷，这次的CVPR之旅注定是我难忘的回忆。对于会议盛况的介绍，网络上也能找到很多相关的文章了，我们还是回到论文上吧。平心而论，可能是由于论文录取数的增加，部分录取论文的水平实在是出乎意料的一般。默默地吐槽一句，两篇 best paper 的论文更是让人感觉有些失望。当然，在逛poster的时候，我还是发现了不少有趣的闪光点的。通过poster展示的方式让作者和读者进行直接的沟通，相互交流论文中的思路和细节，这是我觉得学术会议最有意思的地方之一。

## 精读篇
----------

1. **Deep Multimodal Representation Learning From Temporal Data** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Deep_Multimodal_Representation_CVPR_2017_paper.html))

哈哈，略有私心，首先介绍的是我们这次录用为poster的论文 -- 《对于时序数据的多模态表示学习》。这是我硕士期间在PARC实习的工作成果之一。其实从现在的角度看，这篇文章的质量也是一般般，由于当初时间比较赶，里面很多实验和可视化分析可以做得更好一些。不过没想到的是，与会的很多人都对我的poster有着浓厚的兴趣，过来围观提问的老师、学术及公司的研究员络绎不绝。我觉得吧，作为至今为止少有的对 *时序数据进行多模特表示学习* 的工作，这篇论文还是有点意义的～
    
首先，多模态学习（Multimodal Learning）顾名思义是对于多模态数据（如图像、文本、视频、音频等）数据进行统一学习的任务。而多模态学习 (Multimodal representation learning）则是更关注于学习一个统一的数据表示（joint representation），这个统一的数据表示既融合了各个模态的信息，而且与各模态的低层数据特性无关，是一个更抽象的统一概念。打个比方，“大笑”作为一个单一的概念，对于图像这个模态而言可以表现为“张开嘴的笑脸”，对于音频则是“笑声”，对于文本则是“大笑”这个词本身。多模态表示学习就是希望从不同模态的表示中学到一个类似“大笑”那样统一概念。然而，多模态表示学习通常并不是一件简单的事情。因为各个模态的低层表示（low-level representation）具有各自迥异的统计特性，以及相互之间非常复杂的非线性关系。近年来，由于深度学习的发展，人们开始用深度网络来先对各个模态提取较为高层的抽象表示（high-level representation），然后再进行多模态融合（multimodal fusion）来实现多模态表示学习。我们的工作的重点在于提出了一个**专门针对多模态时序数据**的多模态表示学习模型，这是前人工作中没有做到的。先放一张我们的海报，大家可以对照着看：
    
<div style="text-align:center;"><img src="/assets/papers/Updated_CorrRNN_poster.pdf" align="center"></div>
    
个人觉得这个工作最有意思的点在于：***我们的模型受启发于多模态时序数据的本质特性（intrinsic characteristic），并且用简单的方法对这些特性进行建模***。或许我们当中用到的一些方法不是十分的创新，但我们的模型中传达了对数据特性本身的深入理解和分析，并且在实际数据的实验中得到检验是有用的。下面我们来简单介绍一下各个模块设计的思路，具体的方法和公式可以阅读我们的论文。
    
- 多模态时序模块（Multimodal GRU Module）
        
    时序数据最显著的特性当然就是它的时序特性。多模态时序模块的目的就是同时地进行 **统一表示** 以及 **时序结构** 的学习。这里我们用了基于一种特殊循环网络（Gated Recurrent Network）的编码-解码框架（Encoder-Decoder），并对当中的神经单元进行适用于多模态的扩展。简单来说，每个单元中会涉及到三部分信息，分别对应两个模态各自的函数变换以及用于得到统一表示的函数变化。由于这里会涉及到一些繁琐的计算公式，感兴趣的朋友可以看论文里具体的描述。
        
- 互相关模块（Correlation Module）
    
    这个模块启发与我们对多模态时序数据特性的发现：在很多的应用场景中，多模态时序数据之间的差异，往往是由于 *使用不同的设备记录同样的事情* 导致的。比如同样记录一个人在跑步，视频、音频和运动传感器都会得到不同模态的数据，但实际上这些数据都是记录着同样的事情。这个发现意味着：**多模态时序数据之间具有很强的相关性**！这对于很多非时序的多模态数据是不适用的。比如说对于图像和文本，它们之间的相关性来源于他们高层次语义层面的相关，跟多模态时序数据间的相关性是截然不同的。因此，我们在模型中加入了互相关模块，希望通过显式地增加各模块函数变换的相关性，从而得到更好的统一表示。这种最大化变换相关性的思路在以往 Canonical Correlation Analysis (CCA) 和 Correlational Neural Network 中也有用到。
        
- 动态加权模块（Dynamic weighting Module）
        
    对于时序数据而言，一个常见的难题是数据的噪声水平（noise level）会随着时间的发展而动态地改变。对此我们加入了动态加权模块，隐性地对各个模态的噪声水平进行评估，然后得到各个模态的置信度权重，并根据这个权重进行加权地多模态融合。这里用到的方法借鉴了近几年十分热门的关注度模型（attention model），利用一个双边线性函数（bilienar function）得到各个模态的“关注程度”。

将这些模块整合到一起，便得到了我们最终的模型 -- 互相关循环神经网络（Correlational Recurrent Neural Network）。这是一个通用的多模态时序数据表示学习框架，可以应用于不同的模态，不同的数据集，不同的任务当中。在我们论文的实验里，我们尝试了 视频-运动传感器 和 视频-音频 这两种不同的多模态数据组合，并应用到了动作识别、语音识别等不同的任务当中。在 视频-运动传感器 的实验当中，我们展示了不同损失函数对模型效果的贡献。尤其是对于跨模态预测的任务（cross modality learning 和 share representation learning， 详见论文），加入了跨模态重构以及互相关模块使模型的性能得到大幅度的提升。在 视频-音频 的实验当中，我们证实了我们的模型优于其他的基于深度学习的多模态表示学习方法。更有趣的是，我们的模型对于噪声具有更好的鲁棒性，这一点在海报最后一个表格中可以看出。



## 闪光篇
----------

### 无监督学习

1. **Learning Features by Watching Objects Move** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Pathak_Learning_Features_by_CVPR_2017_paper.html))

2. **Self-Supervised Video Representation Learning With Odd-One-Out Networks** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Fernando_Self-Supervised_Video_Representation_CVPR_2017_paper.html))

3. **Deep Unsupervised Similarity Learning Using Partially Ordered Sets** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Bautista_Deep_Unsupervised_Similarity_CVPR_2017_paper.html))

4. **Unsupervised Learning of Depth and Ego-Motion From Video** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html))

5. **Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.html))

### 视频理解

1. **Predictive-Corrective Networks for Action Detection** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Dave_Predictive-Corrective_Networks_for_CVPR_2017_paper.html))

2. **CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos** ([link](CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos))

3. **Temporal Action Localization by Structured Maximal Sums** ([link](Temporal Action Localization by Structured Maximal Sums))

### GAN

1. **Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Nguyen_Plug__Play_CVPR_2017_paper.html))

2. **Neural Face Editing With Intrinsic Image Disentangling** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Shu_Neural_Face_Editing_CVPR_2017_paper.html))

### 其它

1. **Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Patrini_Making_Deep_Neural_CVPR_2017_paper.html))

2. **Network Dissection: Quantifying Interpretability of Deep Visual Representations** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.html))

3. **Object Region Mining With Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Wei_Object_Region_Mining_CVPR_2017_paper.html))

4. **Top-Down Visual Saliency Guided by Captions** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.html))

5. **Additive Component Analysis** ([link](http://openaccess.thecvf.com/content_cvpr_2017/html/Murdock_Additive_Component_Analysis_CVPR_2017_paper.html))
